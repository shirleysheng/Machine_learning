{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_SVM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMxiwdzwETQsU99VrDTVQr6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shirleysheng/Machine_learning/blob/master/ML_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c37zxZ2KOzgd",
        "colab_type": "text"
      },
      "source": [
        "Based on the definition of Support vector machine, I am trying to build a svm classify method by myself.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "As we know, the constraint of svm is $y_i( \\omega x_i ) + b -1 > 0$ (=0 stand for on the decision boundary) And we want to maximize the width between the boundaries, which withd = $\\frac{2}{\\omega}$ equals to minimize the $\\frac{1}{2}\\omega$. So in order to find the minimum $\\omega$, we used the conves optimization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwP2VgmUOwsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk4FFMhaQuWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class support_vector_machine:\n",
        "\n",
        "  def __init__(self, visualization = True):\n",
        "    self.visualization = visualization \n",
        "    self.color = {1: 'r', -1:'b' }\n",
        "    if self.visualization:\n",
        "      self.fig = plt.figure()\n",
        "      self.ax = self.fig.add_subplot(1,1,1)\n",
        "    \n",
        "  #create training process\n",
        "  def fit (self, data):\n",
        "    self.data = data\n",
        "    opt = {}\n",
        "    transforms = [[1,1],[-1,1],[1,-1],[-1,-1]]\n",
        "\n",
        "    all_data = []\n",
        "    for yi in self.data:\n",
        "      for featureset in self.data[yi]:\n",
        "        for features in featureset:\n",
        "          all_data.append(features)\n",
        "\n",
        "    self.max_feature = max(all_data)\n",
        "    self.min_feature = min(all_data)\n",
        "\n",
        "    all_data =  None #clear\n",
        "\n",
        "    step_size = [self.max_feature * 0.1, self.max_feature * 0.01, self.max_feature * 0.001]\n",
        "\n",
        "\n",
        "    #set b (very expensive)\n",
        "    b_range = 5\n",
        "    b_multiple = 5# do not need to do samll step as same as w\n",
        "\n",
        "    last_opti = self.max_feature * 10\n",
        "\n",
        "    for step in step_size:\n",
        "      w = np.array([last_opti,last_opti])\n",
        "\n",
        "      optimize  = False\n",
        "\n",
        "      while not optimize:\n",
        "        for b in np.arange(-1 * self.max_feature* b_range, self.max_feature*b_range,step *b_multiple):\n",
        "          for transform in transforms:\n",
        "            w_t = w * transform\n",
        "            found_option = True\n",
        "\n",
        "            for i in self.data:\n",
        "              for x in self.data[i]:\n",
        "                yi = i\n",
        "                if not yi * (np.dot(w_t,x) + b) >= 1: #our constraint \n",
        "                  found_option = False\n",
        "            \n",
        "            if found_option:\n",
        "              opt[np.linalg.norm(w_t)] = [w_t, b]\n",
        "        \n",
        "        if w[0] <0:\n",
        "          optimize = True\n",
        "        else:\n",
        "          w = w- step # warning\n",
        "\n",
        "\n",
        "        norms = sorted([n for n in opt])\n",
        "        opt_choice = opt[norms[0]]\n",
        "        self.w = opt_choice[0]\n",
        "        self.b = opt_choice[1]\n",
        "        last_opti = opt_choice[0][0] + step *2\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n49hir18l4LP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}